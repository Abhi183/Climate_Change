{"cells":[{"cell_type":"code","execution_count":null,"id":"5e39212c","metadata":{"id":"5e39212c"},"outputs":[],"source":["import os\n","import numpy as np \n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import math\n","from sklearn.cluster import KMeans \n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from matplotlib import pyplot as plt\n","from matplotlib import animation, colors\n","from sklearn.cluster import KMeans\n","from sklearn import preprocessing as sk_preprocessing\n","from sklearn.metrics import accuracy_score #scoring\n","from sklearn.metrics import confusion_matrix\n","from sklearn.decomposition import PCA\n","from numpy.ma.core import ceil\n","from scipy.spatial import distance #distance calculation\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras import datasets, layers, models\n","from keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"id":"6e87e4f9","metadata":{"id":"6e87e4f9"},"outputs":[],"source":["pressure = '/Users/votri/Downloads/DSDA 385 Machine Learning/Final Project - Climate Change/Pressure/MinMaxNormWithLabel/'\n","\n","# Get the list of subfolders\n","subfolders = [subfolder for subfolder in os.listdir(pressure) if os.path.isdir(os.path.join(pressure, subfolder))]\n","\n","dfs = []\n","reshaped_arrays = []\n","folder_names = []\n","\n","for subfolder in subfolders:\n","    subfolder_path = os.path.join(pressure, subfolder)\n","    files = [f for f in os.listdir(subfolder_path) if f.endswith('.csv')]\n","    for file in files:\n","        csv_file_path = os.path.join(subfolder_path, file)\n","        df = pd.read_csv(csv_file_path, header=None)\n","        dfs.append(df)\n","        \n","        reshaped_array = np.reshape(df.values, (1, -1)) # Reshape the dataframe into a 1D array with 225 attributes\n","        reshaped_arrays.append(reshaped_array)  # Append the reshaped array to the list\n","        \n","        folder_names.append(subfolder) # Save the class of the image\n","        \n","# Concatenate the reshaped arrays into a single array\n","concatenated_array = np.concatenate(reshaped_arrays, axis=0)\n","\n","# Convert the concatenated array into a dataframe\n","result_df = pd.DataFrame(concatenated_array)\n","result_df['Class'] = folder_names"]},{"cell_type":"code","execution_count":null,"id":"8eb99cf0","metadata":{"scrolled":true,"id":"8eb99cf0","outputId":"f99585d3-41ad-4186-b68c-01b3d7112f36"},"outputs":[{"name":"stdout","output_type":"stream","text":["            0         1         2         3         4         5         6  \\\n","0    0.139692  0.139077  0.137846  0.137231  0.137231  0.137231  0.137846   \n","1    0.475692  0.473846  0.471385  0.468923  0.465846  0.462769  0.460308   \n","2    0.248615  0.243692  0.240000  0.237538  0.235692  0.234462  0.234462   \n","3    0.248615  0.243692  0.240000  0.237538  0.235692  0.234462  0.234462   \n","4    0.293538  0.284923  0.278154  0.272615  0.268923  0.265231  0.262769   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","732  0.844923  0.857231  0.872615  0.886154  0.890462  0.892308  0.890462   \n","733  0.912615  0.915692  0.923692  0.929846  0.932308  0.935385  0.936615   \n","734  0.931077  0.933538  0.931692  0.931692  0.931077  0.929231  0.928615   \n","735  0.868923  0.875077  0.880000  0.883692  0.884923  0.884923  0.883692   \n","736  0.856000  0.864000  0.874462  0.876923  0.873846  0.864000  0.868923   \n","\n","            7         8         9  ...       216       217       218  \\\n","0    0.138462  0.140308  0.142154  ...  0.096615  0.089846  0.084308   \n","1    0.456615  0.453538  0.450462  ...  0.425846  0.424000  0.422769   \n","2    0.235692  0.237538  0.240615  ...  0.228308  0.220308  0.216000   \n","3    0.235692  0.237538  0.240615  ...  0.228308  0.220308  0.216000   \n","4    0.262154  0.261538  0.262769  ...  0.224615  0.217231  0.213538   \n","..        ...       ...       ...  ...       ...       ...       ...   \n","732  0.887385  0.881846  0.872615  ...  0.899692  0.895385  0.892308   \n","733  0.937231  0.937231  0.936615  ...  0.937231  0.934769  0.932923   \n","734  0.926769  0.926769  0.929846  ...  0.929846  0.924308  0.921231   \n","735  0.881231  0.878769  0.875077  ...  0.873846  0.876923  0.877538   \n","736  0.872615  0.880615  0.889846  ...  0.936615  0.936615  0.937231   \n","\n","          219       220       221       222       223       224  Class  \n","0    0.081231  0.080000  0.082462  0.086154  0.092308  0.102769     CL  \n","1    0.422154  0.423385  0.425846  0.429538  0.435077  0.443077     CL  \n","2    0.214154  0.216615  0.222769  0.232615  0.246154  0.264615     CL  \n","3    0.214154  0.216615  0.222769  0.232615  0.246154  0.264615     CL  \n","4    0.213538  0.216000  0.221538  0.229538  0.240000  0.253538     CL  \n","..        ...       ...       ...       ...       ...       ...    ...  \n","732  0.889846  0.887385  0.889231  0.896000  0.899077  0.900308   NROI  \n","733  0.929846  0.927385  0.926154  0.926769  0.927385  0.928615   NROI  \n","734  0.920615  0.921846  0.921846  0.923077  0.920615  0.921846   NROI  \n","735  0.881846  0.887385  0.892923  0.898462  0.904000  0.913846   NROI  \n","736  0.937846  0.936615  0.935385  0.933538  0.931077  0.927385   NROI  \n","\n","[737 rows x 226 columns]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>215</th>\n","      <th>216</th>\n","      <th>217</th>\n","      <th>218</th>\n","      <th>219</th>\n","      <th>220</th>\n","      <th>221</th>\n","      <th>222</th>\n","      <th>223</th>\n","      <th>224</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>...</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","      <td>737.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.653576</td>\n","      <td>0.652917</td>\n","      <td>0.652001</td>\n","      <td>0.650932</td>\n","      <td>0.649845</td>\n","      <td>0.648673</td>\n","      <td>0.647626</td>\n","      <td>0.646720</td>\n","      <td>0.646113</td>\n","      <td>0.645782</td>\n","      <td>...</td>\n","      <td>0.649868</td>\n","      <td>0.644940</td>\n","      <td>0.641662</td>\n","      <td>0.640246</td>\n","      <td>0.640848</td>\n","      <td>0.643169</td>\n","      <td>0.647061</td>\n","      <td>0.652169</td>\n","      <td>0.658171</td>\n","      <td>0.664773</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.259836</td>\n","      <td>0.261524</td>\n","      <td>0.262980</td>\n","      <td>0.264169</td>\n","      <td>0.265151</td>\n","      <td>0.265783</td>\n","      <td>0.266180</td>\n","      <td>0.266394</td>\n","      <td>0.266370</td>\n","      <td>0.266135</td>\n","      <td>...</td>\n","      <td>0.277650</td>\n","      <td>0.278376</td>\n","      <td>0.278900</td>\n","      <td>0.279298</td>\n","      <td>0.279582</td>\n","      <td>0.279557</td>\n","      <td>0.279201</td>\n","      <td>0.278348</td>\n","      <td>0.277009</td>\n","      <td>0.275190</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.483692</td>\n","      <td>0.480000</td>\n","      <td>0.478769</td>\n","      <td>0.473846</td>\n","      <td>0.468308</td>\n","      <td>0.468308</td>\n","      <td>0.467692</td>\n","      <td>0.464615</td>\n","      <td>0.464000</td>\n","      <td>0.464615</td>\n","      <td>...</td>\n","      <td>0.459077</td>\n","      <td>0.451692</td>\n","      <td>0.448000</td>\n","      <td>0.444308</td>\n","      <td>0.444923</td>\n","      <td>0.447385</td>\n","      <td>0.451077</td>\n","      <td>0.459692</td>\n","      <td>0.467692</td>\n","      <td>0.478154</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.719385</td>\n","      <td>0.721231</td>\n","      <td>0.721231</td>\n","      <td>0.716923</td>\n","      <td>0.710769</td>\n","      <td>0.711385</td>\n","      <td>0.712615</td>\n","      <td>0.705231</td>\n","      <td>0.704000</td>\n","      <td>0.701538</td>\n","      <td>...</td>\n","      <td>0.719385</td>\n","      <td>0.710154</td>\n","      <td>0.707077</td>\n","      <td>0.707077</td>\n","      <td>0.710769</td>\n","      <td>0.709538</td>\n","      <td>0.713231</td>\n","      <td>0.719385</td>\n","      <td>0.726154</td>\n","      <td>0.736000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.882462</td>\n","      <td>0.888615</td>\n","      <td>0.894769</td>\n","      <td>0.899692</td>\n","      <td>0.901538</td>\n","      <td>0.903385</td>\n","      <td>0.902769</td>\n","      <td>0.900923</td>\n","      <td>0.898462</td>\n","      <td>0.898462</td>\n","      <td>...</td>\n","      <td>0.910769</td>\n","      <td>0.907077</td>\n","      <td>0.902769</td>\n","      <td>0.899692</td>\n","      <td>0.900308</td>\n","      <td>0.903385</td>\n","      <td>0.910154</td>\n","      <td>0.916308</td>\n","      <td>0.918769</td>\n","      <td>0.922462</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.999385</td>\n","      <td>0.995692</td>\n","      <td>0.992615</td>\n","      <td>0.988923</td>\n","      <td>...</td>\n","      <td>0.990769</td>\n","      <td>0.994462</td>\n","      <td>0.996923</td>\n","      <td>0.996923</td>\n","      <td>0.996923</td>\n","      <td>0.995077</td>\n","      <td>0.992000</td>\n","      <td>0.988308</td>\n","      <td>0.986462</td>\n","      <td>0.987692</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 225 columns</p>\n","</div>"],"text/plain":["              0           1           2           3           4           5    \\\n","count  737.000000  737.000000  737.000000  737.000000  737.000000  737.000000   \n","mean     0.653576    0.652917    0.652001    0.650932    0.649845    0.648673   \n","std      0.259836    0.261524    0.262980    0.264169    0.265151    0.265783   \n","min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n","25%      0.483692    0.480000    0.478769    0.473846    0.468308    0.468308   \n","50%      0.719385    0.721231    0.721231    0.716923    0.710769    0.711385   \n","75%      0.882462    0.888615    0.894769    0.899692    0.901538    0.903385   \n","max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n","\n","              6           7           8           9    ...         215  \\\n","count  737.000000  737.000000  737.000000  737.000000  ...  737.000000   \n","mean     0.647626    0.646720    0.646113    0.645782  ...    0.649868   \n","std      0.266180    0.266394    0.266370    0.266135  ...    0.277650   \n","min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n","25%      0.467692    0.464615    0.464000    0.464615  ...    0.459077   \n","50%      0.712615    0.705231    0.704000    0.701538  ...    0.719385   \n","75%      0.902769    0.900923    0.898462    0.898462  ...    0.910769   \n","max      0.999385    0.995692    0.992615    0.988923  ...    0.990769   \n","\n","              216         217         218         219         220         221  \\\n","count  737.000000  737.000000  737.000000  737.000000  737.000000  737.000000   \n","mean     0.644940    0.641662    0.640246    0.640848    0.643169    0.647061   \n","std      0.278376    0.278900    0.279298    0.279582    0.279557    0.279201   \n","min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n","25%      0.451692    0.448000    0.444308    0.444923    0.447385    0.451077   \n","50%      0.710154    0.707077    0.707077    0.710769    0.709538    0.713231   \n","75%      0.907077    0.902769    0.899692    0.900308    0.903385    0.910154   \n","max      0.994462    0.996923    0.996923    0.996923    0.995077    0.992000   \n","\n","              222         223         224  \n","count  737.000000  737.000000  737.000000  \n","mean     0.652169    0.658171    0.664773  \n","std      0.278348    0.277009    0.275190  \n","min      0.000000    0.000000    0.000000  \n","25%      0.459692    0.467692    0.478154  \n","50%      0.719385    0.726154    0.736000  \n","75%      0.916308    0.918769    0.922462  \n","max      0.988308    0.986462    0.987692  \n","\n","[8 rows x 225 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["print(result_df)\n","result_df.describe()"]},{"cell_type":"code","execution_count":null,"id":"aac4950f","metadata":{"scrolled":true,"id":"aac4950f","outputId":"7122cd52-ae16-4505-b466-3811ecbee916"},"outputs":[{"name":"stdout","output_type":"stream","text":["COL     327\n","CL      178\n","COH     161\n","NROI     71\n","Name: Class, dtype: int64\n"]}],"source":["counts = result_df['Class'].value_counts()\n","print(counts)"]},{"cell_type":"code","execution_count":null,"id":"a0f7f8e2","metadata":{"scrolled":true,"id":"a0f7f8e2","outputId":"f9fbed52-91c7-4f58-d3ac-95f2fabfda74"},"outputs":[{"name":"stdout","output_type":"stream","text":["0      0\n","1      0\n","2      0\n","3      0\n","4      0\n","      ..\n","732    1\n","733    1\n","734    1\n","735    1\n","736    1\n","Name: Class, Length: 737, dtype: int64\n"]}],"source":["X = result_df.drop('Class', axis = 1)\n","Y = result_df['Class']\n","\n","Y_numerized = Y.replace({'CL': 0, 'COL' : 2, 'COH': 3, 'NROI': 1})\n","print(Y_numerized)"]},{"cell_type":"code","execution_count":null,"id":"0fd48510","metadata":{"id":"0fd48510","outputId":"6ed80770-4a44-4b2e-c858-c72791d63763"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[[[0.139692]\n","   [0.139077]\n","   [0.137846]\n","   ...\n","   [0.150154]\n","   [0.153846]\n","   [0.156923]]\n","\n","  [[0.136   ]\n","   [0.134154]\n","   [0.131692]\n","   ...\n","   [0.138462]\n","   [0.142154]\n","   [0.146462]]\n","\n","  [[0.132308]\n","   [0.129846]\n","   [0.126154]\n","   ...\n","   [0.124923]\n","   [0.129846]\n","   [0.134769]]\n","\n","  ...\n","\n","  [[0.150154]\n","   [0.138462]\n","   [0.125538]\n","   ...\n","   [0.069538]\n","   [0.075692]\n","   [0.084308]]\n","\n","  [[0.158769]\n","   [0.146462]\n","   [0.133538]\n","   ...\n","   [0.076923]\n","   [0.081846]\n","   [0.091692]]\n","\n","  [[0.167385]\n","   [0.155692]\n","   [0.142769]\n","   ...\n","   [0.086154]\n","   [0.092308]\n","   [0.102769]]]\n","\n","\n"," [[[0.475692]\n","   [0.473846]\n","   [0.471385]\n","   ...\n","   [0.446769]\n","   [0.448   ]\n","   [0.450462]]\n","\n","  [[0.473846]\n","   [0.470769]\n","   [0.467692]\n","   ...\n","   [0.437538]\n","   [0.44    ]\n","   [0.443077]]\n","\n","  [[0.468923]\n","   [0.464615]\n","   [0.460308]\n","   ...\n","   [0.428308]\n","   [0.430769]\n","   [0.435692]]\n","\n","  ...\n","\n","  [[0.427077]\n","   [0.423385]\n","   [0.419077]\n","   ...\n","   [0.402462]\n","   [0.408615]\n","   [0.417846]]\n","\n","  [[0.432615]\n","   [0.428923]\n","   [0.426462]\n","   ...\n","   [0.414154]\n","   [0.420308]\n","   [0.428923]]\n","\n","  [[0.436923]\n","   [0.435692]\n","   [0.433846]\n","   ...\n","   [0.429538]\n","   [0.435077]\n","   [0.443077]]]\n","\n","\n"," [[[0.248615]\n","   [0.243692]\n","   [0.24    ]\n","   ...\n","   [0.253538]\n","   [0.260308]\n","   [0.267692]]\n","\n","  [[0.246154]\n","   [0.24    ]\n","   [0.235077]\n","   ...\n","   [0.244923]\n","   [0.251692]\n","   [0.259692]]\n","\n","  [[0.243692]\n","   [0.236308]\n","   [0.230154]\n","   ...\n","   [0.236308]\n","   [0.243692]\n","   [0.251692]]\n","\n","  ...\n","\n","  [[0.302769]\n","   [0.276923]\n","   [0.254769]\n","   ...\n","   [0.201231]\n","   [0.214154]\n","   [0.230154]]\n","\n","  [[0.324923]\n","   [0.297231]\n","   [0.273231]\n","   ...\n","   [0.214769]\n","   [0.227692]\n","   [0.244923]]\n","\n","  [[0.349538]\n","   [0.321846]\n","   [0.296   ]\n","   ...\n","   [0.232615]\n","   [0.246154]\n","   [0.264615]]]\n","\n","\n"," ...\n","\n","\n"," [[[0.931077]\n","   [0.933538]\n","   [0.931692]\n","   ...\n","   [0.930462]\n","   [0.931077]\n","   [0.931692]]\n","\n","  [[0.933538]\n","   [0.933538]\n","   [0.933538]\n","   ...\n","   [0.932923]\n","   [0.934154]\n","   [0.934154]]\n","\n","  [[0.935385]\n","   [0.934154]\n","   [0.933538]\n","   ...\n","   [0.933538]\n","   [0.933538]\n","   [0.935385]]\n","\n","  ...\n","\n","  [[0.930462]\n","   [0.931692]\n","   [0.932923]\n","   ...\n","   [0.917538]\n","   [0.921846]\n","   [0.925538]]\n","\n","  [[0.929846]\n","   [0.933538]\n","   [0.933538]\n","   ...\n","   [0.921846]\n","   [0.921231]\n","   [0.922462]]\n","\n","  [[0.933538]\n","   [0.937231]\n","   [0.936615]\n","   ...\n","   [0.923077]\n","   [0.920615]\n","   [0.921846]]]\n","\n","\n"," [[[0.868923]\n","   [0.875077]\n","   [0.88    ]\n","   ...\n","   [0.862154]\n","   [0.858462]\n","   [0.855385]]\n","\n","  [[0.879385]\n","   [0.884308]\n","   [0.888   ]\n","   ...\n","   [0.858462]\n","   [0.855385]\n","   [0.854154]]\n","\n","  [[0.888615]\n","   [0.891692]\n","   [0.893538]\n","   ...\n","   [0.856   ]\n","   [0.854769]\n","   [0.855385]]\n","\n","  ...\n","\n","  [[0.900923]\n","   [0.892308]\n","   [0.884308]\n","   ...\n","   [0.883692]\n","   [0.888615]\n","   [0.894769]]\n","\n","  [[0.899077]\n","   [0.891077]\n","   [0.882462]\n","   ...\n","   [0.891077]\n","   [0.897231]\n","   [0.904   ]]\n","\n","  [[0.897846]\n","   [0.889846]\n","   [0.881846]\n","   ...\n","   [0.898462]\n","   [0.904   ]\n","   [0.913846]]]\n","\n","\n"," [[[0.856   ]\n","   [0.864   ]\n","   [0.874462]\n","   ...\n","   [0.897231]\n","   [0.896615]\n","   [0.895385]]\n","\n","  [[0.852308]\n","   [0.851692]\n","   [0.856615]\n","   ...\n","   [0.899077]\n","   [0.900923]\n","   [0.902154]]\n","\n","  [[0.854154]\n","   [0.855385]\n","   [0.859077]\n","   ...\n","   [0.905846]\n","   [0.907692]\n","   [0.907692]]\n","\n","  ...\n","\n","  [[0.913231]\n","   [0.916923]\n","   [0.921231]\n","   ...\n","   [0.931077]\n","   [0.928615]\n","   [0.924923]]\n","\n","  [[0.918769]\n","   [0.922462]\n","   [0.925538]\n","   ...\n","   [0.932308]\n","   [0.929846]\n","   [0.926154]]\n","\n","  [[0.923077]\n","   [0.926154]\n","   [0.929231]\n","   ...\n","   [0.933538]\n","   [0.931077]\n","   [0.927385]]]]\n"]}],"source":["X_2D = X.to_numpy().reshape(X.shape[0], 15, 15, 1)\n","print(X_2D)"]},{"cell_type":"code","execution_count":null,"id":"6575ce80","metadata":{"id":"6575ce80","outputId":"5444de67-adf9-49c2-d029-21087abc3fb3"},"outputs":[{"name":"stdout","output_type":"stream","text":["(515, 15, 15, 1) (515,) (222, 15, 15, 1) (222,)\n"]}],"source":["# train and test split\n","train_x, test_x, train_y, test_y = train_test_split(X_2D, Y_numerized, test_size=0.3, random_state=42)\n","print(train_x.shape, train_y.shape, test_x.shape, test_y.shape) # check the shapes"]},{"cell_type":"code","execution_count":null,"id":"647e32ba","metadata":{"scrolled":true,"id":"647e32ba"},"outputs":[],"source":["model = Sequential()\n","model.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=(15,15,1)))\n","model.add(Conv2D(filters=64, kernel_size=3, activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(4, activation='softmax'))\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"d99f8968","metadata":{"scrolled":true,"id":"d99f8968","outputId":"c0f7c54c-be6d-40a4-ead6-ef3be55a626e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n","17/17 [==============================] - 1s 22ms/step - loss: 1.3014 - accuracy: 0.4252\n","Epoch 2/1000\n","17/17 [==============================] - 0s 23ms/step - loss: 1.2008 - accuracy: 0.5184\n","Epoch 3/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 1.0291 - accuracy: 0.5417\n","Epoch 4/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.9071 - accuracy: 0.5767\n","Epoch 5/1000\n","17/17 [==============================] - 0s 19ms/step - loss: 0.9768 - accuracy: 0.5728\n","Epoch 6/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 1.1178 - accuracy: 0.5495\n","Epoch 7/1000\n","17/17 [==============================] - 0s 19ms/step - loss: 0.9879 - accuracy: 0.5767\n","Epoch 8/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.8935 - accuracy: 0.5961\n","Epoch 9/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.9099 - accuracy: 0.5534\n","Epoch 10/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.8374 - accuracy: 0.6583\n","Epoch 11/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.9624 - accuracy: 0.5806\n","Epoch 12/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.9480 - accuracy: 0.5165\n","Epoch 13/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.8736 - accuracy: 0.5767\n","Epoch 14/1000\n","17/17 [==============================] - 0s 19ms/step - loss: 0.8542 - accuracy: 0.5961\n","Epoch 15/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.8401 - accuracy: 0.6194\n","Epoch 16/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.8227 - accuracy: 0.6155\n","Epoch 17/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.8056 - accuracy: 0.6369\n","Epoch 18/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.7920 - accuracy: 0.6311\n","Epoch 19/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.8391 - accuracy: 0.6214\n","Epoch 20/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.8153 - accuracy: 0.6117\n","Epoch 21/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.7807 - accuracy: 0.6408\n","Epoch 22/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.7760 - accuracy: 0.6311\n","Epoch 23/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.8063 - accuracy: 0.6233\n","Epoch 24/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.7673 - accuracy: 0.6252\n","Epoch 25/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.8440 - accuracy: 0.5961\n","Epoch 26/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.7942 - accuracy: 0.6350\n","Epoch 27/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.8098 - accuracy: 0.6408\n","Epoch 28/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.7832 - accuracy: 0.6466\n","Epoch 29/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.8594 - accuracy: 0.6058\n","Epoch 30/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.7799 - accuracy: 0.6447\n","Epoch 31/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.7431 - accuracy: 0.6621\n","Epoch 32/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.7653 - accuracy: 0.6544\n","Epoch 33/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.7196 - accuracy: 0.6544\n","Epoch 34/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.7080 - accuracy: 0.6971\n","Epoch 35/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.7115 - accuracy: 0.6660\n","Epoch 36/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.8081 - accuracy: 0.6485\n","Epoch 37/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.7409 - accuracy: 0.6466\n","Epoch 38/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.7720 - accuracy: 0.6660\n","Epoch 39/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.6819 - accuracy: 0.6796\n","Epoch 40/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.7195 - accuracy: 0.6272\n","Epoch 41/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.8026 - accuracy: 0.6369\n","Epoch 42/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.7225 - accuracy: 0.6621\n","Epoch 43/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.6692 - accuracy: 0.6777\n","Epoch 44/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.7932 - accuracy: 0.6175\n","Epoch 45/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.6961 - accuracy: 0.6913\n","Epoch 46/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.6928 - accuracy: 0.6621\n","Epoch 47/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.6024 - accuracy: 0.7379\n","Epoch 48/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.6817 - accuracy: 0.6816\n","Epoch 49/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.5904 - accuracy: 0.7592\n","Epoch 50/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.5963 - accuracy: 0.7476\n","Epoch 51/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.5367 - accuracy: 0.7786\n","Epoch 52/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.5218 - accuracy: 0.7417\n","Epoch 53/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.4964 - accuracy: 0.7767\n","Epoch 54/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.6128 - accuracy: 0.7223\n","Epoch 55/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.5249 - accuracy: 0.7631\n","Epoch 56/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.4508 - accuracy: 0.7981\n","Epoch 57/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.4264 - accuracy: 0.8194\n","Epoch 58/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.4400 - accuracy: 0.8175\n","Epoch 59/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.6550 - accuracy: 0.7107\n","Epoch 60/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.4834 - accuracy: 0.7845\n","Epoch 61/1000\n","17/17 [==============================] - 0s 20ms/step - loss: 0.4042 - accuracy: 0.8039\n","Epoch 62/1000\n","17/17 [==============================] - 0s 19ms/step - loss: 0.4268 - accuracy: 0.8058\n","Epoch 63/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.4210 - accuracy: 0.8272\n","Epoch 64/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3797 - accuracy: 0.8447\n","Epoch 65/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3641 - accuracy: 0.8427\n","Epoch 66/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.4167 - accuracy: 0.8136\n","Epoch 67/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.4001 - accuracy: 0.8291\n","Epoch 68/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.4048 - accuracy: 0.8097\n","Epoch 69/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.4426 - accuracy: 0.8136\n","Epoch 70/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3841 - accuracy: 0.8388\n","Epoch 71/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3845 - accuracy: 0.8388\n","Epoch 72/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3620 - accuracy: 0.8466\n","Epoch 73/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3472 - accuracy: 0.8544\n","Epoch 74/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3572 - accuracy: 0.8505\n","Epoch 75/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3351 - accuracy: 0.8485\n","Epoch 76/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.4078 - accuracy: 0.8330\n","Epoch 77/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3708 - accuracy: 0.8330\n","Epoch 78/1000\n","17/17 [==============================] - 0s 19ms/step - loss: 0.3620 - accuracy: 0.8447\n","Epoch 79/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3670 - accuracy: 0.8311\n","Epoch 80/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3575 - accuracy: 0.8485\n","Epoch 81/1000\n"]},{"name":"stdout","output_type":"stream","text":["17/17 [==============================] - 0s 16ms/step - loss: 0.3602 - accuracy: 0.8408\n","Epoch 82/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3580 - accuracy: 0.8466\n","Epoch 83/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3515 - accuracy: 0.8388\n","Epoch 84/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3488 - accuracy: 0.8311\n","Epoch 85/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3749 - accuracy: 0.8408\n","Epoch 86/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3472 - accuracy: 0.8408\n","Epoch 87/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3410 - accuracy: 0.8408\n","Epoch 88/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3234 - accuracy: 0.8718\n","Epoch 89/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3323 - accuracy: 0.8544\n","Epoch 90/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3423 - accuracy: 0.8524\n","Epoch 91/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3406 - accuracy: 0.8408\n","Epoch 92/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3236 - accuracy: 0.8563\n","Epoch 93/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3948 - accuracy: 0.8194\n","Epoch 94/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.4250 - accuracy: 0.8369\n","Epoch 95/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.4260 - accuracy: 0.8214\n","Epoch 96/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3828 - accuracy: 0.8136\n","Epoch 97/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3314 - accuracy: 0.8427\n","Epoch 98/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3419 - accuracy: 0.8466\n","Epoch 99/1000\n","17/17 [==============================] - 0s 15ms/step - loss: 0.3966 - accuracy: 0.8330\n","Epoch 100/1000\n","17/17 [==============================] - 0s 15ms/step - loss: 0.3413 - accuracy: 0.8447\n","Epoch 101/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3583 - accuracy: 0.8388\n","Epoch 102/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3329 - accuracy: 0.8641\n","Epoch 103/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3239 - accuracy: 0.8427\n","Epoch 104/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3218 - accuracy: 0.8602\n","Epoch 105/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3260 - accuracy: 0.8563\n","Epoch 106/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3200 - accuracy: 0.8621\n","Epoch 107/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3920 - accuracy: 0.8136\n","Epoch 108/1000\n","17/17 [==============================] - 0s 15ms/step - loss: 0.3547 - accuracy: 0.8485\n","Epoch 109/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3637 - accuracy: 0.8350\n","Epoch 110/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3489 - accuracy: 0.8408\n","Epoch 111/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3201 - accuracy: 0.8680\n","Epoch 112/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3481 - accuracy: 0.8505\n","Epoch 113/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3297 - accuracy: 0.8447\n","Epoch 114/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3327 - accuracy: 0.8602\n","Epoch 115/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3303 - accuracy: 0.8485\n","Epoch 116/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3383 - accuracy: 0.8563\n","Epoch 117/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3330 - accuracy: 0.8505\n","Epoch 118/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3368 - accuracy: 0.8466\n","Epoch 119/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3232 - accuracy: 0.8699\n","Epoch 120/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3219 - accuracy: 0.8621\n","Epoch 121/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3249 - accuracy: 0.8544\n","Epoch 122/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.4492 - accuracy: 0.7981\n","Epoch 123/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3498 - accuracy: 0.8660\n","Epoch 124/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3449 - accuracy: 0.8505\n","Epoch 125/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.4471 - accuracy: 0.8175\n","Epoch 126/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3855 - accuracy: 0.8408\n","Epoch 127/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3432 - accuracy: 0.8427\n","Epoch 128/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3342 - accuracy: 0.8369\n","Epoch 129/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3400 - accuracy: 0.8272\n","Epoch 130/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3166 - accuracy: 0.8660\n","Epoch 131/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3195 - accuracy: 0.8563\n","Epoch 132/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3349 - accuracy: 0.8524\n","Epoch 133/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.5052 - accuracy: 0.7961\n","Epoch 134/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3526 - accuracy: 0.8388\n","Epoch 135/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3319 - accuracy: 0.8602\n","Epoch 136/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3484 - accuracy: 0.8505\n","Epoch 137/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3373 - accuracy: 0.8583\n","Epoch 138/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3174 - accuracy: 0.8544\n","Epoch 139/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3760 - accuracy: 0.8427\n","Epoch 140/1000\n","17/17 [==============================] - 0s 15ms/step - loss: 0.3407 - accuracy: 0.8583\n","Epoch 141/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3699 - accuracy: 0.8388\n","Epoch 142/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3219 - accuracy: 0.8602\n","Epoch 143/1000\n","17/17 [==============================] - 0s 15ms/step - loss: 0.3334 - accuracy: 0.8641\n","Epoch 144/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3329 - accuracy: 0.8757\n","Epoch 145/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3558 - accuracy: 0.8388\n","Epoch 146/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3293 - accuracy: 0.8602\n","Epoch 147/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3061 - accuracy: 0.8621\n","Epoch 148/1000\n","17/17 [==============================] - 0s 15ms/step - loss: 0.3110 - accuracy: 0.8641\n","Epoch 149/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3199 - accuracy: 0.8524\n","Epoch 150/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3062 - accuracy: 0.8524\n","Epoch 151/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2982 - accuracy: 0.8680\n","Epoch 152/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3099 - accuracy: 0.8641\n","Epoch 153/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3153 - accuracy: 0.8621\n","Epoch 154/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3099 - accuracy: 0.8738\n","Epoch 155/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3069 - accuracy: 0.8680\n","Epoch 156/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3073 - accuracy: 0.8757\n","Epoch 157/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2945 - accuracy: 0.8835\n","Epoch 158/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2935 - accuracy: 0.8796\n","Epoch 159/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3001 - accuracy: 0.8680\n","Epoch 160/1000\n"]},{"name":"stdout","output_type":"stream","text":["17/17 [==============================] - 0s 16ms/step - loss: 0.3109 - accuracy: 0.8602\n","Epoch 161/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2845 - accuracy: 0.8699\n","Epoch 162/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3056 - accuracy: 0.8641\n","Epoch 163/1000\n","17/17 [==============================] - 0s 15ms/step - loss: 0.2906 - accuracy: 0.8718\n","Epoch 164/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3287 - accuracy: 0.8544\n","Epoch 165/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3181 - accuracy: 0.8699\n","Epoch 166/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3143 - accuracy: 0.8583\n","Epoch 167/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.4973 - accuracy: 0.8000\n","Epoch 168/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.4146 - accuracy: 0.8194\n","Epoch 169/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3232 - accuracy: 0.8563\n","Epoch 170/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3129 - accuracy: 0.8757\n","Epoch 171/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3012 - accuracy: 0.8660\n","Epoch 172/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3007 - accuracy: 0.8602\n","Epoch 173/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3535 - accuracy: 0.8388\n","Epoch 174/1000\n","17/17 [==============================] - 0s 19ms/step - loss: 0.3070 - accuracy: 0.8563\n","Epoch 175/1000\n","17/17 [==============================] - 0s 20ms/step - loss: 0.3214 - accuracy: 0.8524\n","Epoch 176/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.2881 - accuracy: 0.8738\n","Epoch 177/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3200 - accuracy: 0.8466\n","Epoch 178/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3022 - accuracy: 0.8602\n","Epoch 179/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3032 - accuracy: 0.8718\n","Epoch 180/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2888 - accuracy: 0.8835\n","Epoch 181/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3205 - accuracy: 0.8621\n","Epoch 182/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3037 - accuracy: 0.8680\n","Epoch 183/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3063 - accuracy: 0.8718\n","Epoch 184/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2902 - accuracy: 0.8738\n","Epoch 185/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3186 - accuracy: 0.8699\n","Epoch 186/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3233 - accuracy: 0.8447\n","Epoch 187/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3118 - accuracy: 0.8680\n","Epoch 188/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.2838 - accuracy: 0.8816\n","Epoch 189/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2843 - accuracy: 0.8874\n","Epoch 190/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2824 - accuracy: 0.8777\n","Epoch 191/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2856 - accuracy: 0.8835\n","Epoch 192/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2835 - accuracy: 0.8816\n","Epoch 193/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3237 - accuracy: 0.8660\n","Epoch 194/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2903 - accuracy: 0.8757\n","Epoch 195/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2835 - accuracy: 0.8641\n","Epoch 196/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2856 - accuracy: 0.8738\n","Epoch 197/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2844 - accuracy: 0.8796\n","Epoch 198/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2804 - accuracy: 0.8854\n","Epoch 199/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3298 - accuracy: 0.8738\n","Epoch 200/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3227 - accuracy: 0.8563\n","Epoch 201/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.5206 - accuracy: 0.8039\n","Epoch 202/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3931 - accuracy: 0.8350\n","Epoch 203/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3371 - accuracy: 0.8505\n","Epoch 204/1000\n","17/17 [==============================] - 0s 19ms/step - loss: 0.3052 - accuracy: 0.8680\n","Epoch 205/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3250 - accuracy: 0.8641\n","Epoch 206/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3103 - accuracy: 0.8544\n","Epoch 207/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2881 - accuracy: 0.8699\n","Epoch 208/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3169 - accuracy: 0.8544\n","Epoch 209/1000\n","17/17 [==============================] - 0s 19ms/step - loss: 0.2966 - accuracy: 0.8680\n","Epoch 210/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3037 - accuracy: 0.8563\n","Epoch 211/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2972 - accuracy: 0.8680\n","Epoch 212/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2941 - accuracy: 0.8602\n","Epoch 213/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.2796 - accuracy: 0.8738\n","Epoch 214/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2804 - accuracy: 0.8796\n","Epoch 215/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.2812 - accuracy: 0.8680\n","Epoch 216/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.2689 - accuracy: 0.8854\n","Epoch 217/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2915 - accuracy: 0.8738\n","Epoch 218/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2992 - accuracy: 0.8602\n","Epoch 219/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3031 - accuracy: 0.8757\n","Epoch 220/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.2971 - accuracy: 0.8621\n","Epoch 221/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3259 - accuracy: 0.8447\n","Epoch 222/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3211 - accuracy: 0.8699\n","Epoch 223/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2873 - accuracy: 0.8777\n","Epoch 224/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2841 - accuracy: 0.8680\n","Epoch 225/1000\n","17/17 [==============================] - 0s 19ms/step - loss: 0.2886 - accuracy: 0.8738\n","Epoch 226/1000\n","17/17 [==============================] - 0s 19ms/step - loss: 0.3146 - accuracy: 0.8718\n","Epoch 227/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3127 - accuracy: 0.8738\n","Epoch 228/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3243 - accuracy: 0.8583\n","Epoch 229/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3101 - accuracy: 0.8544\n","Epoch 230/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3061 - accuracy: 0.8505\n","Epoch 231/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3267 - accuracy: 0.8466\n","Epoch 232/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3113 - accuracy: 0.8699\n","Epoch 233/1000\n","17/17 [==============================] - 0s 19ms/step - loss: 0.3008 - accuracy: 0.8738\n","Epoch 234/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.2852 - accuracy: 0.8738\n","Epoch 235/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3111 - accuracy: 0.8427\n","Epoch 236/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2897 - accuracy: 0.8738\n","Epoch 237/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2989 - accuracy: 0.8641\n","Epoch 238/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3195 - accuracy: 0.8505\n","Epoch 239/1000\n"]},{"name":"stdout","output_type":"stream","text":["17/17 [==============================] - 0s 16ms/step - loss: 0.2927 - accuracy: 0.8699\n","Epoch 240/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2808 - accuracy: 0.8777\n","Epoch 241/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2883 - accuracy: 0.8660\n","Epoch 242/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2764 - accuracy: 0.8796\n","Epoch 243/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2720 - accuracy: 0.8699\n","Epoch 244/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2907 - accuracy: 0.8641\n","Epoch 245/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3144 - accuracy: 0.8544\n","Epoch 246/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3162 - accuracy: 0.8777\n","Epoch 247/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3144 - accuracy: 0.8738\n","Epoch 248/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.2807 - accuracy: 0.8738\n","Epoch 249/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2636 - accuracy: 0.8913\n","Epoch 250/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2861 - accuracy: 0.8718\n","Epoch 251/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3168 - accuracy: 0.8641\n","Epoch 252/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3420 - accuracy: 0.8544\n","Epoch 253/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3017 - accuracy: 0.8621\n","Epoch 254/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.2746 - accuracy: 0.8796\n","Epoch 255/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.2806 - accuracy: 0.8816\n","Epoch 256/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2652 - accuracy: 0.8796\n","Epoch 257/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2845 - accuracy: 0.8874\n","Epoch 258/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3033 - accuracy: 0.8738\n","Epoch 259/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2885 - accuracy: 0.8757\n","Epoch 260/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3173 - accuracy: 0.8447\n","Epoch 261/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3304 - accuracy: 0.8466\n","Epoch 262/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2940 - accuracy: 0.8718\n","Epoch 263/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3023 - accuracy: 0.8408\n","Epoch 264/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3041 - accuracy: 0.8699\n","Epoch 265/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3197 - accuracy: 0.8583\n","Epoch 266/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.2956 - accuracy: 0.8699\n","Epoch 267/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2834 - accuracy: 0.8738\n","Epoch 268/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2773 - accuracy: 0.8738\n","Epoch 269/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.2740 - accuracy: 0.8913\n","Epoch 270/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2850 - accuracy: 0.8854\n","Epoch 271/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2792 - accuracy: 0.8699\n","Epoch 272/1000\n","17/17 [==============================] - 0s 15ms/step - loss: 0.2690 - accuracy: 0.8796\n","Epoch 273/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2673 - accuracy: 0.8893\n","Epoch 274/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2961 - accuracy: 0.8796\n","Epoch 275/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3434 - accuracy: 0.8485\n","Epoch 276/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3718 - accuracy: 0.8524\n","Epoch 277/1000\n","17/17 [==============================] - 0s 18ms/step - loss: 0.3435 - accuracy: 0.8583\n","Epoch 278/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3530 - accuracy: 0.8583\n","Epoch 279/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3106 - accuracy: 0.8602\n","Epoch 280/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2877 - accuracy: 0.8874\n","Epoch 281/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2846 - accuracy: 0.8757\n","Epoch 282/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2911 - accuracy: 0.8777\n","Epoch 283/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2783 - accuracy: 0.8777\n","Epoch 284/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2883 - accuracy: 0.8641\n","Epoch 285/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2822 - accuracy: 0.8816\n","Epoch 286/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2857 - accuracy: 0.8835\n","Epoch 287/1000\n","17/17 [==============================] - 0s 19ms/step - loss: 0.3461 - accuracy: 0.8524\n","Epoch 288/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3169 - accuracy: 0.8660\n","Epoch 289/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3114 - accuracy: 0.8447\n","Epoch 290/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2876 - accuracy: 0.8699\n","Epoch 291/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.4528 - accuracy: 0.8175\n","Epoch 292/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2984 - accuracy: 0.8660\n","Epoch 293/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3106 - accuracy: 0.8544\n","Epoch 294/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3319 - accuracy: 0.8621\n","Epoch 295/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2834 - accuracy: 0.8835\n","Epoch 296/1000\n","17/17 [==============================] - 0s 17ms/step - loss: 0.3403 - accuracy: 0.8466\n","Epoch 297/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2812 - accuracy: 0.8660\n","Epoch 298/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.3259 - accuracy: 0.8583\n","Epoch 299/1000\n","17/17 [==============================] - 0s 16ms/step - loss: 0.2849 - accuracy: 0.8796\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1dc2450ad60>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Train the model\n","early_stopping = EarlyStopping(monitor='loss', patience=50)\n","model.fit(train_x, train_y, epochs=1000, batch_size=32, callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":null,"id":"1b7c5822","metadata":{"id":"1b7c5822","outputId":"f2a041f7-322f-41cd-c051-1d986dedb07e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test accuracy: 0.8693693693693694\n"]}],"source":["# Use the trained model to make predictions on the test set\n","y_pred = model.predict(test_x).argmax(axis=1)\n","\n","# Calculate the accuracy of the model on the test set\n","accuracy = accuracy_score(test_y, y_pred)\n","print('Test accuracy:', accuracy)"]},{"cell_type":"code","execution_count":null,"id":"a8037d73","metadata":{"id":"a8037d73","outputId":"51767eb6-5c8a-444e-85e3-8c39ff0cdc2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[52  0  6  0]\n"," [ 0 12  7  2]\n"," [ 8  3 81  0]\n"," [ 0  2  1 48]]\n"]}],"source":["cm = confusion_matrix(test_y, y_pred)\n","print(cm)"]},{"cell_type":"code","execution_count":null,"id":"b9c73b05","metadata":{"id":"b9c73b05"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}